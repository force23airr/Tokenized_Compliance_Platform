{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RWA Compliance AI - Model Fine-Tuning\n",
        "\n",
        "This notebook demonstrates fine-tuning open-source LLMs for compliance tasks.\n",
        "\n",
        "## Models\n",
        "- **Mistral 7B** - Primary model for compliance reasoning\n",
        "- **Legal-BERT** - For document classification\n",
        "\n",
        "## Tasks\n",
        "1. Jurisdiction Classification\n",
        "2. Conflict Resolution\n",
        "3. Document Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install torch transformers datasets peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Base Model with Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = 'mistralai/Mistral-7B-v0.1'\n",
        "\n",
        "# Quantization config for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f'Tokenizer loaded: {MODEL_NAME}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print('Model loaded and prepared for training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                          # Rank\n",
        "    lora_alpha=32,                 # Alpha scaling\n",
        "    lora_dropout=0.05,             # Dropout\n",
        "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],  # Attention layers\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample training data for jurisdiction classification\n",
        "training_data = [\n",
        "    {\n",
        "        'instruction': 'Analyze the following document and determine the investor jurisdiction and classification.',\n",
        "        'input': 'W-9 form submitted. SSN ending 5678. Address: 123 Main Street, New York, NY 10001. Annual income reported: $275,000 for 2023 and $260,000 for 2022.',\n",
        "        'output': json.dumps({\n",
        "            'jurisdiction': 'US',\n",
        "            'entity_type': 'individual',\n",
        "            'classification': 'accredited',\n",
        "            'reasoning': 'US person (W-9 + NY address). Qualifies as accredited investor under SEC Rule 501(a) - income exceeds $200K threshold for 2 consecutive years.',\n",
        "            'applicable_regulations': ['SEC Reg D 501(a)', 'FINRA Rule 5123']\n",
        "        })\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Analyze the following document and determine the investor jurisdiction and classification.',\n",
        "        'input': 'Certificate of Incorporation from Companies House. Company number: 12345678. Registered office: 100 Bishopsgate, London EC2N 4AG. Latest accounts show net assets of £15,000,000.',\n",
        "        'output': json.dumps({\n",
        "            'jurisdiction': 'UK',\n",
        "            'entity_type': 'corporation',\n",
        "            'classification': 'professional',\n",
        "            'reasoning': 'UK company (Companies House registration). Qualifies as per se professional client under MiFID II - large undertaking with net assets exceeding €2M threshold.',\n",
        "            'applicable_regulations': ['FCA COBS 3.5', 'MiFID II Annex II']\n",
        "        })\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Analyze the following document and determine the investor jurisdiction and classification.',\n",
        "        'input': 'ACRA Business Profile. UEN: 202312345K. Entity name: ABC Capital Pte Ltd. Registered address: 1 Raffles Place, Singapore 048616. Paid-up capital: S$15,000,000.',\n",
        "        'output': json.dumps({\n",
        "            'jurisdiction': 'SG',\n",
        "            'entity_type': 'corporation',\n",
        "            'classification': 'accredited',\n",
        "            'reasoning': 'Singapore company (ACRA registration). Qualifies as accredited investor under SFA Section 4A(1)(a) - corporation with net assets exceeding S$10M.',\n",
        "            'applicable_regulations': ['SFA Section 4A', 'SFA Section 275']\n",
        "        })\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f'Training samples: {len(training_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format for instruction tuning\n",
        "def format_prompt(sample):\n",
        "    return f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Input:\n",
        "{sample['input']}\n",
        "\n",
        "### Response:\n",
        "{sample['output']}\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(sample):\n",
        "    prompt = format_prompt(sample)\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    result['labels'] = result['input_ids'].copy()\n",
        "    return result\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(training_data)\n",
        "tokenized_dataset = dataset.map(tokenize)\n",
        "\n",
        "print(f'Dataset prepared: {len(tokenized_dataset)} samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='../models/jurisdiction-classifier/checkpoints',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    save_strategy='epoch',\n",
        "    fp16=True,\n",
        "    report_to='tensorboard'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "print('Trainer configured')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training (uncomment to run)\n",
        "# trainer.train()\n",
        "\n",
        "print('Training would start here - uncomment trainer.train() to execute')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save fine-tuned model (uncomment after training)\n",
        "# OUTPUT_DIR = '../models/jurisdiction-classifier/final'\n",
        "# trainer.save_model(OUTPUT_DIR)\n",
        "# tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "# print(f'Model saved to {OUTPUT_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a new sample\n",
        "test_input = \"\"\"### Instruction:\n",
        "Analyze the following document and determine the investor jurisdiction and classification.\n",
        "\n",
        "### Input:\n",
        "Cayman Islands Certificate of Incorporation. Company number: MC-12345. Registered office: PO Box 309, George Town, Grand Cayman. Fund documents indicate this is a private investment fund with $50M AUM.\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "inputs = tokenizer(test_input, return_tensors='pt').to(model.device)\n",
        "\n",
        "# Generate response\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Expand training data to 1000+ samples\n",
        "2. Add conflict resolution training\n",
        "3. Add document generation training\n",
        "4. Evaluate on benchmark test cases\n",
        "5. Deploy to inference API"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
